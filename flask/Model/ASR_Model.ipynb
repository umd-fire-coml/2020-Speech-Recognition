{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ASR_Model.ipynb","provenance":[],"collapsed_sections":["C6paxkD19WqC"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"F5-n1M3-ljdz"},"source":["# ASR Model\n","\n","In this notebook we create a Automatic Speech Recognition Model based on [Mozilla's DeepSpeech Model](https://github.com/mozilla/DeepSpeech) and [Baidu's Deep Speech Paper](https://arxiv.org/abs/1412.5567). A lot of the code in this notebook is based on the source code of the [deepspeech-keras library](https://github.com/val260/DeepSpeech-Keras) and in some cases will directly parallel that code (we would undoubtedly fail a plagarism checker and that's why we would like to be upfront with our sources). Developing this notebook, however, required us to \n","* understand how each component of the DeepSpeech model works,\n","* migrate a library distributed over many files into a single notebook,\n","* and convert code from Keras to Tensorflow 2 Keras"]},{"cell_type":"markdown","metadata":{"id":"b7UT6LMzhWHj"},"source":["## Link To Drive\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Nx5m_IL7ne-9"},"source":["There are a few files that we will need to run this notebook. These should be located in Model directory within our repository. If you have been following our setup instructions in the Github.ipynb file, you won't have to change any of the file paths below."]},{"cell_type":"code","metadata":{"id":"c0wkXBy8hY4Q","executionInfo":{"status":"ok","timestamp":1605127282880,"user_tz":300,"elapsed":31071,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"29d80b22-17b6-47fc-9325-5c3296ff47e0","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rN2lHaVfhjNU","executionInfo":{"status":"ok","timestamp":1605127287095,"user_tz":300,"elapsed":2227,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"94d8c485-1948-4168-e142-3980862308dc","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd \"/content/drive/My Drive/Research/FIRE/2020-Speech-Recognition/Model\"\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Research/FIRE/2020-Speech-Recognition/Model\n","alphabet.py\t callbacks.py\t     data\t__pycache__\ttensorboard\n","alphabet.txt\t checkpoints\t     hello.m4a\tresults.bin\tyellow.m4a\n","ASR_Model.ipynb  configuration.yaml  model.png\tStarWars60.wav\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6LFBjNj1u4Er"},"source":["## Install Required Libraries\n","\n","As long as you are using Google Colab, the only required library is the python_speech_features library. All other libraries come included with Google Colab. In Tensorflow 2.3, some of the functionality was deprecated, so we also rollback the tensorflow version to 2.1.0."]},{"cell_type":"code","metadata":{"id":"mFDIGRdoqB_r"},"source":["!pip install python_speech_features\n","!pip install tensorflow==2.1.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2iQHIk3u6es"},"source":["## Encode Audio\n","\n","To perform speech recognition our audio data, we want to first preprocess each audio file to provide us with input features. The code blocks below extract the features from a given audio file using librosa and python_speech_features. We also run this code on a sample audio file with 60 seconds of the Stars Wars theme to test our ability to create features from audio data."]},{"cell_type":"code","metadata":{"id":"0ws4zAzXp_sg","executionInfo":{"status":"ok","timestamp":1605118557914,"user_tz":300,"elapsed":110901,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["import numpy as np\n","import python_speech_features\n","import librosa\n","\n","conf = {\"winfunc\": np.hamming, \"winlen\": 0.025, \"winstep\": .01, \"nfilt\": 80}\n","\n","def get_features(files):\n","    mfccs = [make_features(file) for file in files]\n","    X = align(mfccs)\n","    return X\n","\n","def make_features(file_path):\n","    \"\"\" Use `python_speech_\"features` lib to extract MFCC features from the audio file. \"\"\"\n","    audio, fs = librosa.load(file_path, sr=16000)\n","    audio = (audio * 32768).astype(\"int16\")\n","    feat, energy = python_speech_features.fbank(audio, samplerate=fs, **conf)\n","    features = np.log(feat)\n","    return features\n","\n","def align(arrays, default=0):\n","    \"\"\" Pad arrays along time dimensions. Return the single array (batch_size, time, features). \"\"\"\n","    max_array = max(arrays, key=len)\n","    X = np.full(shape=[len(arrays), *max_array.shape], fill_value=default, dtype=np.float64)\n","    for index, array in enumerate(arrays):\n","        time_dim, features_dim = array.shape\n","        X[index, :time_dim] = array\n","    return X"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqNiEP3PrNn6","executionInfo":{"status":"ok","timestamp":1605118561843,"user_tz":300,"elapsed":110244,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"0685b798-556a-4bdb-a70c-348c7096061d","colab":{"base_uri":"https://localhost:8080/"}},"source":["X = get_features([\"StarWars60.wav\"])\n","X.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 5999, 80)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"B9diInnS6nfA"},"source":["## Alphabet\n","\n","The next step is to get our alphabet. The alphabet defines what letters are included in our classified text. Our alphabet file includes 36 characters right now, but we may want to reduce this to only the 26 letters and an empty space from the English alphabet. "]},{"cell_type":"code","metadata":{"id":"D0eFRDPh7OEm","executionInfo":{"status":"ok","timestamp":1605118562397,"user_tz":300,"elapsed":109538,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["from alphabet import Alphabet\n","alphabet = Alphabet(\"alphabet.txt\")"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o-Vubqsiv057"},"source":["## Create Model\n","\n","In the code cells below, we create the model that we will be using for Speech To Text Recognition. "]},{"cell_type":"code","metadata":{"id":"uT0Tg1pX0IoE","executionInfo":{"status":"ok","timestamp":1605118564985,"user_tz":300,"elapsed":109322,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["from tensorflow import config\n","list_physical_devices = config.list_physical_devices\n","model_dir = \"models/\"\n","gpus = list_physical_devices(\"GPU\")"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oC0G5Wmn2Inn"},"source":["### Layers\n","\n","To build the model, we use the Functional API provided by Tensorflow Keras. The code cells below build the model based on the structure of DeepSpeech. There are also some optimizations included for whether there are gpus being utilized for this task."]},{"cell_type":"code","metadata":{"id":"LPj44zGhxEPs","executionInfo":{"status":"ok","timestamp":1605118564987,"user_tz":300,"elapsed":102738,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["from typing import List\n","from tensorflow.keras import Model\n","import tensorflow\n","#from keras.initializers import np\n","from tensorflow import expand_dims, squeeze\n","from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n","from tensorflow.keras.layers import Input, Lambda, LSTM, Bidirectional, Dense, ReLU, \\\n","    TimeDistributed, BatchNormalization, Dropout, ZeroPadding2D, Conv2D, Reshape"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgsR6s9NxFJD","executionInfo":{"status":"ok","timestamp":1605120919544,"user_tz":300,"elapsed":2398,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"88a3e13d-2a4c-4aba-cb44-e89792d1423b","colab":{"base_uri":"https://localhost:8080/"}},"source":["def get_model():\n","    input_dim = 80\n","    is_gpu = len(gpus) > 0\n","    output_dim = 28\n","    context = 7\n","    units = 1024\n","    dropouts = [0.1, .1, 0]\n","    #random_state = 1\n","\n","    #np.random.seed(1)\n","    #tensorflow.random.set_seed(random_state)\n","    input_tensor = Input([None, input_dim], name='X')                           # Define input tensor [time, features]\n","    x = Lambda(expand_dims, arguments=dict(axis=-1))(input_tensor)              # Add 4th dim (channel)\n","    x = ZeroPadding2D(padding=(context, 0))(x)                                  # Fill zeros around time dimension\n","    receptive_field = (2*context + 1, input_dim)                                # Take into account fore/back-ward context\n","    x = Conv2D(filters=units, kernel_size=receptive_field)(x)                   # Convolve signal in time dim\n","    x = Lambda(squeeze, arguments=dict(axis=2))(x)                              # Squeeze into 3rd dim array\n","    x = ReLU(max_value=20)(x)                                                   # Add non-linearity\n","    x = Dropout(rate=dropouts[0])(x)                                            # Use dropout as regularization\n","\n","    x = TimeDistributed(Dense(units))(x)                                        # 2nd and 3rd FC layers do a feature\n","    x = ReLU(max_value=20)(x)                                                   # extraction base on the context\n","    x = Dropout(rate=dropouts[1])(x)\n","\n","    x = TimeDistributed(Dense(units))(x)\n","    x = ReLU(max_value=20)(x)\n","    x = Dropout(rate=dropouts[2])(x)\n","\n","    x = Bidirectional(CuDNNLSTM(units, return_sequences=True) if is_gpu else     # LSTM handle long dependencies\n","                        LSTM(units, return_sequences=True, ),\n","                        merge_mode='sum')(x)\n","\n","    output_tensor = TimeDistributed(Dense(output_dim, activation='softmax'))(x)  # Return at each time step prob along characters\n","\n","    model = Model(inputs=input_tensor, outputs=output_tensor)\n","    return model\n","\n","model = get_model()\n","model.summary()"],"execution_count":59,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","X (InputLayer)               [(None, None, 80)]        0         \n","_________________________________________________________________\n","lambda_8 (Lambda)            (None, None, 80, 1)       0         \n","_________________________________________________________________\n","zero_padding2d_4 (ZeroPaddin (None, None, 80, 1)       0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, None, 1, 1024)     1229824   \n","_________________________________________________________________\n","lambda_9 (Lambda)            (None, None, 1024)        0         \n","_________________________________________________________________\n","re_lu_12 (ReLU)              (None, None, 1024)        0         \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, None, 1024)        0         \n","_________________________________________________________________\n","time_distributed_12 (TimeDis (None, None, 1024)        1049600   \n","_________________________________________________________________\n","re_lu_13 (ReLU)              (None, None, 1024)        0         \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, None, 1024)        0         \n","_________________________________________________________________\n","time_distributed_13 (TimeDis (None, None, 1024)        1049600   \n","_________________________________________________________________\n","re_lu_14 (ReLU)              (None, None, 1024)        0         \n","_________________________________________________________________\n","dropout_14 (Dropout)         (None, None, 1024)        0         \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, None, 1024)        16793600  \n","_________________________________________________________________\n","time_distributed_14 (TimeDis (None, None, 28)          28700     \n","=================================================================\n","Total params: 20,151,324\n","Trainable params: 20,151,324\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tYTEq6uI2Frs"},"source":["### Loss Function\n","\n","For our loss function, we use a CTC Loss. The CTC loss helps with the problem of alignment where we don't know which portion of the audio corresponds to which letter (just that certain audio corresponds to certain phrases or words). This loss is described in detail [here](https://distill.pub/2017/ctc/). "]},{"cell_type":"code","metadata":{"id":"yA-tVw8-12ku","executionInfo":{"status":"ok","timestamp":1605118568329,"user_tz":300,"elapsed":98108,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["import tensorflow as tf\n","def ctc_loss(y, y_hat):\n","    print(\"calculating loss\")\n","    def get_length(tensor):\n","        lengths = tf.reduce_sum(tf.ones_like(tensor), 1)\n","        return tf.reshape(tf.cast(lengths, tf.int32), [-1, 1])\n","\n","\n","    sequence_length = get_length(tf.reduce_max(y_hat, 2))\n","    label_length = get_length(y)\n","    ret = tf.keras.backend.ctc_batch_cost(y, y_hat, sequence_length, label_length)\n","    print(ret)\n","    return ret\n","\n","loss = ctc_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xfjWaao02OKT"},"source":["### Optimizer\n","\n","For our optimizer, we use the standard Adam Optimzier. The configurations below are the default configurations for an Adam Optimizer but can be tuned in the future."]},{"cell_type":"code","metadata":{"id":"myBPy3LT2NZl","executionInfo":{"status":"ok","timestamp":1605118568330,"user_tz":300,"elapsed":95713,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["from tensorflow.keras.optimizers import Optimizer, SGD, Adam\n","\n","optimizer = Adam(\n","    learning_rate=0.001,\n","    beta_1=0.9,\n","    beta_2=0.999,\n","    epsilon=1e-07,\n","    amsgrad=False,\n","    name=\"Adam\",\n",")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"neMi_vntkEiD"},"source":["### Compile The Model\n","\n","We have enough components at this point to compile our model with the given optimizer and loss. The DeepSpeech Keras model also included target_tensors in their compiled model. I don't have a good enough understanding of target_tensors to state what they do, but they have been discontinued starting Tensorflow 2.2.0. Currently the lines for adding target_tensors have been commented out, but we may need to consider how to fix this issue of deprecation."]},{"cell_type":"code","metadata":{"id":"6g2buO3v5O2-","executionInfo":{"status":"ok","timestamp":1605120929581,"user_tz":300,"elapsed":344,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"7fcfa013-e328-4db4-b89c-157ad020a50f","colab":{"base_uri":"https://localhost:8080/"}},"source":["from tensorflow.keras.utils import multi_gpu_model\n","#print(tf.__version__)\n","gpus_num = len(gpus)\n","compiled_model = multi_gpu_model(model, gpus_num) if gpus_num > 1 else model\n","y = Input(name='y', shape=[None], dtype='int32')\n","compiled_model.compile(optimizer, loss, target_tensors=[y])\n","#compiled_model.template_model = model"],"execution_count":60,"outputs":[{"output_type":"stream","text":["calculating loss\n","Tensor(\"loss_4/time_distributed_14_loss/ExpandDims:0\", shape=(None, 1), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uz6VyrUr3xUg"},"source":["### Test The Model\n","\n","We can also test our model on the sample audio file that we had extracted features from earlier. The decoder has not been set up yet so we take a rudimentary approach of removing all consecutive duplicates. We don't expect this model to classify anything at this point since it has been created with random weights, but this allows us to add a sanity check to see that our model is processing our input features and creating an output vector of probabilities for each character. "]},{"cell_type":"code","metadata":{"id":"27F2ZBI_4t0r","executionInfo":{"status":"ok","timestamp":1605120932067,"user_tz":300,"elapsed":689,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"75388a69-5209-4c9d-95c4-340378dd1036","colab":{"base_uri":"https://localhost:8080/"}},"source":["y_hat = compiled_model.predict_on_batch(X)\n","arr = [alphabet.string_from_label(l) for l in np.apply_along_axis(np.argmax, 1, y_hat[0])]\n","arr = ''.join([arr[0]] + [arr[i] for i in range(1,len(arr)) if arr[i] != arr[i-1]])\n","print(arr)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["qdqdqdqd\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QQCiM2m_24Hn"},"source":["## Callbacks"]},{"cell_type":"code","metadata":{"id":"MHy5bDhG23it","executionInfo":{"status":"ok","timestamp":1605120583093,"user_tz":300,"elapsed":520,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["from tensorflow.keras.callbacks import Callback, TerminateOnNaN, LearningRateScheduler, ReduceLROnPlateau, History\n","import importlib\n","import callbacks\n","importlib.reload(callbacks)\n","from callbacks import ResultKeeper, CustomModelCheckpoint, CustomTensorBoard, CustomEarlyStopping\n","callbacks = []\n","callbacks.append(TerminateOnNaN())\n","callbacks.append(ResultKeeper(\"results.bin\"))\n","callbacks.append(CustomModelCheckpoint('checkpoints'))\n","callbacks.append(CustomTensorBoard('tensorboard'))\n","callbacks.append(CustomEarlyStopping(mini_targets={5: 200, 10:100}, monitor=\"val_loss\", patience=3))\n","#lr_decay = lambda epoch, lr: lr / np.power(.1, epoch)\n","#callbacks.append(LearningRateScheduler(lr_decay, verbose= 1))"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbhCmFG-Chrh"},"source":["## Train The Model"]},{"cell_type":"code","metadata":{"id":"7EB45NWuDIoI","executionInfo":{"status":"ok","timestamp":1605120471969,"user_tz":300,"elapsed":2067,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"a0c90c4a-57ae-429b-e8ea-54e8639c6f17","colab":{"base_uri":"https://localhost:8080/"}},"source":["X = get_features([\"hello.m4a\", \"yellow.m4a\", \"hello.m4a\", \"yellow.m4a\"])\n","X.shape"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 114, 80)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"tTD37STCDkIn","executionInfo":{"status":"ok","timestamp":1605120474440,"user_tz":300,"elapsed":264,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"73f772d7-6a1d-4afd-8d93-cda2a1989a22","colab":{"base_uri":"https://localhost:8080/"}},"source":["Y = [\"hello\", \"yello\", \"hello\", \"yello\"]\n","labels = alphabet.get_batch_labels(Y)\n","labels"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 8,  5, 12, 12, 15],\n","       [25,  5, 12, 12, 15],\n","       [ 8,  5, 12, 12, 15],\n","       [25,  5, 12, 12, 15]])"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"I0tXXjCBCkMy","executionInfo":{"status":"ok","timestamp":1605121012017,"user_tz":300,"elapsed":75782,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"c61bb60e-fa6b-4556-abc3-6b9f5411d5f3","colab":{"base_uri":"https://localhost:8080/"}},"source":["compiled_model.fit(X,labels,callbacks=callbacks,batch_size=2,epochs=10,validation_split=.5)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Train on 2 samples, validate on 2 samples\n","Epoch 1/10\n","2/2 [==============================] - 6s 3s/sample - loss: 341.8538 - val_loss: 55.7923\n","Epoch 2/10\n","2/2 [==============================] - 16s 8s/sample - loss: 55.7923 - val_loss: 55.7923\n","Epoch 3/10\n","2/2 [==============================] - 17s 9s/sample - loss: 55.7923 - val_loss: 55.7923\n","Epoch 4/10\n","2/2 [==============================] - 25s 13s/sample - loss: 55.7923 - val_loss: 55.7923\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0ed37af908>"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"FoO1DRjJvchs"},"source":["y_hat = compiled_model.predict_on_batch(X)\n","y_hat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C6paxkD19WqC"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"3-dYPZRviKcR","executionInfo":{"status":"ok","timestamp":1605120651431,"user_tz":300,"elapsed":275,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}}},"source":["def batch_tensorflow_decode(y_hat, decoder, alphabet):\n","    \"\"\" Enable to batch decode using tensorflow decoder. \"\"\"\n","    labels, = decoder([y_hat])\n","    return alphabet.get_batch_transcripts(labels)"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Z2-Dd519Ya4","executionInfo":{"status":"ok","timestamp":1605119347906,"user_tz":300,"elapsed":523,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"74c349f2-8c1c-4b33-f6eb-bca2466f6a7b","colab":{"base_uri":"https://localhost:8080/"}},"source":["from functools import partial\n","from tensorflow.keras import backend as K\n","\n","def get_decoder(output_tensor):\n","    def get_length(tensor):\n","        lengths = tf.reduce_sum(tf.ones_like(tensor), 1)\n","        return tf.cast(lengths, tf.int32)\n","\n","    sequence_length = get_length(tf.reduce_max(output_tensor, 2))\n","    top_k_decoded, _ = K.ctc_decode(output_tensor, sequence_length, greedy=False, beam_width=64)\n","    print(top_k_decoded[0])\n","    decoder = K.function([output_tensor], [top_k_decoded[0]])\n","    return decoder\n","\n","print(model.output)\n","decoder = get_decoder(model.output)\n","decoder = partial(batch_tensorflow_decode, alphabet=alphabet, decoder=decoder)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Tensor(\"time_distributed_8/Identity:0\", shape=(None, None, 28), dtype=float32)\n","Tensor(\"SparseToDense_2:0\", shape=(None, None), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rPCsVTslicdX","executionInfo":{"status":"ok","timestamp":1605121363793,"user_tz":300,"elapsed":324,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"9d3bb166-da63-48cd-d1f1-49cb9d64b2bc","colab":{"base_uri":"https://localhost:8080/"}},"source":["decoder(y_hat)"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['l', 'l', 'l', 'l']"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"Pwj4shf4yhnT"},"source":["## Training on A Real Dataset"]},{"cell_type":"code","metadata":{"id":"-YdSL5-mymVE"},"source":["!pip install pydub"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1bSaGJJx_6X","executionInfo":{"status":"ok","timestamp":1604965531674,"user_tz":300,"elapsed":1157,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"49ca8dfc-1e68-4177-a1b0-57ca872aeeff","colab":{"base_uri":"https://localhost:8080/"}},"source":["import tensorflow_datasets as tfds\n","ds = tfds.load('speech_commands', split='train', shuffle_files=True)\n","assert isinstance(ds, tf.data.Dataset)\n","print(ds)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<_OptionsDataset shapes: {audio: (None,), label: ()}, types: {audio: tf.int64, label: tf.int64}>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jzWMFcletC_9"},"source":["import tensorflow_datasets as tfds\n","ds = tfds.load(\"common_voice\",split=\"train\", shuffle_files=True, data_dir=\"./data\")\n","assert isinstance(ds, tf.data.Dataset)\n","print(ds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SHylgDv69RI"},"source":["def make_features_from_audio(audio):\n","    \"\"\" Use `python_speech_\"features` lib to extract MFCC features from the audio file. \"\"\"\n","    #audio, fs = librosa.load(file_path, sr=16000)\n","    audio = audio.astype(\"int16\")\n","    feat, energy = python_speech_features.fbank(audio, samplerate=16000, **conf)\n","    features = np.log(feat)\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kZIpb328ZPQ","executionInfo":{"status":"ok","timestamp":1604965539889,"user_tz":300,"elapsed":206,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"140edd54-914c-4af8-b897-0ce23cb2a268","colab":{"base_uri":"https://localhost:8080/"}},"source":["classes = \"down,go,left,no,off,on,right,stop,up,yes,,\".split(\",\")\n","classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes', '', '']"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"eT8TNAUP24ZN","executionInfo":{"status":"ok","timestamp":1604965772990,"user_tz":300,"elapsed":227785,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"b5c836c3-167e-42e8-cfea-d976f156236b","colab":{"base_uri":"https://localhost:8080/"}},"source":["from tqdm import tqdm\n","\n","gen = ds.as_numpy_iterator()\n","X = []\n","Y = []\n","for e in tqdm(gen):\n","    if e[\"label\"] < 10:\n","        X.append(make_features_from_audio(e[\"audio\"]))\n","        Y.append(classes[e[\"label\"]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["85511it [03:47, 376.09it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Tvim0SsIJnOW"},"source":["labels = alphabet.get_batch_labels(Y)\n","X_align = align(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVrgHpWcJdrJ"},"source":["history = compiled_model.fit(X_align,labels,callbacks=callbacks,batch_size=32,epochs=10,validation_split=.05)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tysASuIzN560","executionInfo":{"status":"ok","timestamp":1604970413193,"user_tz":300,"elapsed":589,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"1369a8e1-9121-4c10-bd1a-90f177806e0a","colab":{"base_uri":"https://localhost:8080/"}},"source":["y_hat = compiled_model.predict_on_batch(X_align[:10])\n","print(Y[:10])\n","decode = lambda y_hat: ''.join([alphabet.string_from_label(l) for l in np.apply_along_axis(np.argmax, 1, y_hat)])\n","reduce = lambda arr: [arr[0]] + [arr[i] for i in range(1,len(arr)) if arr[i] != arr[i-1]]\n","[decode(yh) for yh in y_hat]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['stop', 'down', 'no', 'yes', 'yes', 'left', 'go', 'off', 'stop', 'left']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['', '', '', '', '', '', '', '', '', '']"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"bccuk-OWkHov"},"source":["## Visualize Results"]},{"cell_type":"code","metadata":{"id":"UjOUG8xtkGcr","executionInfo":{"status":"ok","timestamp":1604970420157,"user_tz":300,"elapsed":212,"user":{"displayName":"Sagar Saxena","photoUrl":"","userId":"06645006870975362026"}},"outputId":"1f355c17-6b47-4441-ca66-4331e1bdf386","colab":{"base_uri":"https://localhost:8080/"}},"source":["%load_ext tensorboard"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ta04dBFFkNW1"},"source":["%tensorboard --logdir tensorboard"],"execution_count":null,"outputs":[]}]}